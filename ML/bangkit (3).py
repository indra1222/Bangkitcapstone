# -*- coding: utf-8 -*-
"""bangkit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15IW-SsqstCbM4Bai4lMGJQk4UPy6aM0M
"""

# Install libraries
!pip install tensorflow flask-ngrok pandas numpy scikit-learn

# Import libraries
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import joblib

# Simulasi data geolokasi (longitude, latitude) untuk pengguna dan tukang
def simulate_geolocation(df):
    np.random.seed(42)
    df['User_Longitude'] = np.random.uniform(95, 141, size=len(df))  # Indonesia's longitude range
    df['User_Latitude'] = np.random.uniform(-10, 6, size=len(df))    # Indonesia's latitude range
    df['Tukang_Longitude'] = np.random.uniform(95, 141, size=len(df))
    df['Tukang_Latitude'] = np.random.uniform(-10, 6, size=len(df))
    return df

# Load dataset
from google.colab import files
uploaded = files.upload()

# Baca dataset yang diunggah
file_name = list(uploaded.keys())[0]
df = pd.read_excel(file_name)

# Tambahkan geolokasi
df = simulate_geolocation(df)

# Fitur tambahan: hitung jarak geografis
def haversine_distance(lat1, lon1, lat2, lon2):
    R = 6371  # Radius of Earth in kilometers
    phi1, phi2 = np.radians(lat1), np.radians(lat2)
    delta_phi = np.radians(lat2 - lat1)
    delta_lambda = np.radians(lon2 - lon1)
    a = np.sin(delta_phi/2.0)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(delta_lambda/2.0)**2
    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))

df['Distance'] = haversine_distance(
    df['User_Latitude'], df['User_Longitude'],
    df['Tukang_Latitude'], df['Tukang_Longitude']
)

# Feature selection
features = ['Distance', 'Rate']
target = 'Rate'

X = df[features]
y = df[target]

# Normalize data
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Save scaler
joblib.dump(scaler, "scaler.pkl")

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Build TensorFlow model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1)  # Predict preference score
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Print model summary
print("Model Summary:")
model.summary()

# Train model
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=8)

# Save the trained model
model.save("tukang_recommender.h5")

# Print training results
import matplotlib.pyplot as plt

# Plot training & validation loss
plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot training & validation MAE
plt.figure(figsize=(12, 6))
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.title('Model Mean Absolute Error')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()
plt.show()

print("Model and scaler saved.")

import tensorflow as tf
print(tf.__version__)